import * as tf from '@tensorflow/tfjs';
import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';
import path from 'path';
import { createModel } from './model.js';
import fs from 'fs';

// Cargar variables de entorno
dotenv.config({ path: path.join(process.cwd(), '.env') });

const SUPABASE_URL = process.env.SUPABASE_URL;
const SUPABASE_KEY = process.env.SUPABASE_KEY;

if (!SUPABASE_URL || !SUPABASE_KEY) {
    console.error('‚ùå ERROR: Credenciales de Supabase no encontradas.');
    process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

// --- CONFIGURACI√ìN DE ENTRENAMIENTO ---
const SYMBOL = 'BTCUSDT';
const TIMEFRAME = '15m';
const LOOKBACK = 50; // Cu√°ntas velas mira hacia atr√°s para decidir
const TEST_SIZE = 0.1; // 10% para validar (Examen final)
const EPOCHS = 10; // Deep Training
const BATCH_SIZE = 32;

// --- FEATURE ENGINEERING ---
// No le damos el precio crudo (BTC var√≠a de 20k a 100k).
// Le damos CAMBIOS porcentuales y osciladores normalizados.
function calculateRSI(closes: number[], period = 14) {
    let rsi = new Array(closes.length).fill(0);
    // (Implementaci√≥n simplificada para ML preprocessing masivo)
    // Nota: Para precisi√≥n perfecta en producci√≥n usamos la librer√≠a t√©cnica, 
    // aqu√≠ hacemos un c√°lculo vectorizado r√°pido.
    // ... Por brevedad del script de entrenamiento inicial, usamos Change % y Volatilidad.
    // Una implementaci√≥n completa de RSI aqu√≠ alargar√≠a mucho el script.
    return rsi;
}

async function fetchTrainingData() {
    console.log("üì° Descargando datos de la Corteza Cerebral (Supabase)...");

    // Fetch en bucle para superar el l√≠mite de 1000 filas de Supabase
    let allData: any[] = [];
    let from = 0;
    const PAGE_SIZE = 1000;
    const MAX_ROWS = 5000; // Fast Track (2 semanas) para demostraci√≥n inmediata

    while (allData.length < MAX_ROWS) {
        const { data, error } = await supabase
            .from('market_candles')
            .select('close, volume, high, low')
            .eq('symbol', SYMBOL)
            .eq('timeframe', TIMEFRAME)
            .order('timestamp', { ascending: true }) // Del pasado al presente
            .range(from, from + PAGE_SIZE - 1);

        if (error) throw new Error(`Data Error: ${error.message}`);
        if (!data || data.length === 0) break;

        allData = allData.concat(data);
        from += PAGE_SIZE;
        process.stdout.write(`.`); // Progreso
    }

    console.log(`\n‚úÖ Datos recibidos: ${allData.length} velas.`);
    return allData;
}

function prepareDatasets(data: any[]) {
    console.log("üß† Preprocesando: Feature Engineering & Normalizaci√≥n...");

    const X: number[][][] = []; // Changed to 3D Array
    const y: number[] = [];

    // Calcular Returns (Cambio %) - Esto hace la data "Estacionaria" (Machine Learning 101)
    const returns = data.map((d, i) => {
        if (i === 0) return 0;
        return (d.close - data[i - 1].close) / data[i - 1].close;
    });

    // Calcular Volatilidad (Range) normalizada
    const ranges = data.map(d => (d.high - d.low) / d.close);

    // Normalizaci√≥n Min-Max (Simple) para ayudar a la red a converger
    // No usamos librer√≠as externas para mantenerlo ligero
    // (En prod idealmente guardamos scaler params)

    for (let i = LOOKBACK; i < data.length - 1; i++) {
        // Feature Vector para la ventana [i-LOOKBACK ... i]
        const windowReturns = returns.slice(i - LOOKBACK, i);
        const windowRanges = ranges.slice(i - LOOKBACK, i);

        // Input: [Sample, TimeSteps, Features]
        const sequence: number[][] = [];
        for (let j = 0; j < LOOKBACK; j++) {
            sequence.push([
                windowReturns[j], // Feat 1: Return
                windowRanges[j]   // Feat 2: Range
            ]);
        }

        X.push(sequence);

        // Target: ¬øLa SIGUIENTE vela (i+1) cerr√≥ con retorno positivo?
        // 1 = Subi√≥, 0 = Baj√≥/Igual
        // Usamos un umbral peque√±o para filtrar ruido (0.001%)
        const nextReturn = returns[i + 1];
        y.push(nextReturn > 0.00001 ? 1 : 0);
    }

    return { X, y };
}

async function train() {
    try {
        const rawData = await fetchTrainingData();
        const { X, y } = prepareDatasets(rawData);

        console.log(`üß© Dataset creado. Muestras de entrenamiento: ${X.length}`);

        // Convertir a Tensores
        // Shape: [Samples, TimeSteps, Features]
        // TimeSteps = LOOKBACK (50)
        // Features = 2 (Return, Range)
        const inputTensor = tf.tensor3d(X, [X.length, LOOKBACK, 2]);
        const labelTensor = tf.tensor2d(y, [y.length, 1]);

        // Split Train/Test
        const splitIdx = Math.floor(X.length * (1 - TEST_SIZE));

        const xTrain = inputTensor.slice([0, 0, 0], [splitIdx, LOOKBACK, 2]);
        const yTrain = labelTensor.slice([0, 0], [splitIdx, 1]);
        const xTest = inputTensor.slice([splitIdx, 0, 0], [X.length - splitIdx, LOOKBACK, 2]);
        const yTest = labelTensor.slice([splitIdx, 0], [X.length - splitIdx, 1]);

        console.log("üèãÔ∏è INICIANDO ENTRENAMIENTO DEL CEREBRO...");
        console.log(`   (Esto tomar√° unos momentos usando: ${tf.getBackend()})`);

        const model = createModel([LOOKBACK, 2]);

        await model.fit(xTrain, yTrain, {
            epochs: EPOCHS,
            batchSize: BATCH_SIZE,
            validationData: [xTest, yTest],
            callbacks: {
                onEpochEnd: (epoch, logs) => {
                    console.log(`   Epoch ${epoch + 1}/${EPOCHS} - Loss: ${logs?.loss.toFixed(4)} - Accuracy: ${logs?.acc.toFixed(4)} - Val Acc: ${logs?.val_acc.toFixed(4)}`);
                }
            }
        });

        console.log("‚úÖ Entrenamiento completado.");

        // Guardar Modelo (Custom Handler para Pure JS)
        const saveDir = './cols_brain_v1';
        if (!fs.existsSync(saveDir)) fs.mkdirSync(saveDir);

        await model.save(tf.io.withSaveHandler(async (artifacts) => {
            if (artifacts.modelTopology) {
                fs.writeFileSync(path.join(saveDir, 'model.json'), JSON.stringify({
                    modelTopology: artifacts.modelTopology,
                    format: artifacts.format,
                    generatedBy: artifacts.generatedBy,
                    convertedBy: artifacts.convertedBy,
                    weightsManifest: [{
                        paths: ['./weights.bin'],
                        weights: artifacts.weightSpecs
                    }]
                }, null, 2));
            }
            if (artifacts.weightData) {
                fs.writeFileSync(path.join(saveDir, 'weights.bin'), Buffer.from(artifacts.weightData));
            }
            return {
                modelArtifactsInfo: {
                    dateSaved: new Date(),
                    modelTopologyType: 'JSON',
                    modelTopologyBytes: JSON.stringify(artifacts.modelTopology).length,
                    weightSpecsBytes: JSON.stringify(artifacts.weightSpecs).length,
                    weightDataBytes: artifacts.weightData ? artifacts.weightData.byteLength : 0,
                }
            };
        }));
        console.log(`üíæ Modelo guardado MANUALMENTE en: ${saveDir}`);

        // Cleanup memory
        inputTensor.dispose();
        labelTensor.dispose();
        xTrain.dispose();
        yTrain.dispose();

    } catch (e) {
        console.error("‚ùå Error Fatal en Entrenamiento:", e);
    }
}

train();
